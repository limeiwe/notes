

## http协议

请求头
描述
Host								主机ip地址或域名
User-Agent					客户端相关信息，如果操作系统、浏览器等信息
Accept							指定客户端接收信息类型，如：image./jpg,text/html,application/json
Accept-Charset			 客户端接受的字符集，如gb2312、iso-8859-1
Accept-Encoding		  可接受的内容编码，如gzip
Accept-Language		 接受的语言，如Accept-Language:zh-cn
Authorization			   客户端提供给服务端，进行权限认证的信息
Cookie						   携带的cookie信息
Referer						  当前文档的UR工，即从哪个链接过来的
Content-Type			   请求体内容类型，如Content-Type:application/x-www-form-urlencoded
Content-Length		   数据长度
Cache-Control			 缓存机制，如cache-Control:no-cache
Pragma						 防止页面被缓存，和Cache-Contro1:no-cache作用一样

# 一、爬虫的理解与分类

1、爬虫：通过编写程序模拟浏览器上网，然后让其去互联网上抓取数据的过程

2、爬虫分类：

通用爬虫（抓取系统中重要组成部分，抓取的是一整张页面数据）
聚焦爬虫（抓取页面中特定的局部内容）
增量式爬虫（只抓取网站中更新的数据）
3、爬虫的基本步骤

前期须知：

----用到requests库模拟浏览器请求，需install--requests

----存储用到文件操作，需install--os

----如需爬取json类型数据，需install--json

步骤分析：想要爬取别人的数据，首先要知道目的地，也就是url，其次不能被人发现我们不是真实的浏览器请求，所以就需要伪装UA，然后用requests里面的方法模拟请求，获取response数据，提取我们自己想要的数据，再用文件流操作持久化存储，供后续使用，总结为以下五步骤。

1）指定url

2）UA伪装

3）发起请求，参数处理

4）获取响应数据

5）持久化存储

二、实例爬虫项目分析
需求：爬取国家药监总局化妆品生产许可证相关数据

http://scxk.nmpa.gov.cn:81/xk/

1、需求拆解

1）我们先看一看这个请求过去，是否有接口能否返回我们想要的内容，经过查找没有-----No，而且我们发现他是动态加载数据的，所以没办法用get请求直接获取到数据，所以我们需要找ajax请求



2）但是我们点击详情进入到，有我们想要的许可证相关信息的详情页面，发现每个页面只有标识id不同，所以我们无论如何都需要从主页某个接口获取到id，将它取出来，再拼接到这个url 作为参数爬取我们想要的数据



3）仔细分析详情页面的接口，发现它也是动态加载数据，无法用get请求直接获取到，经过f12，查看接口，我们找到这个接口可以获取到我们想要的详情数据

 



而且他的请求参数只有一个，就是id，机会就来了，所以我们现在重点就是需要找到id，可以将它存储在list中，再读取，作为参数，供下面的接口使用，最终爬取我们想要的数据



4）我们返回首页，f12查看接口，找到这个接口可以获取id数据，符合我们预期，现在所有的有用的资料都已收集完毕，我们开始撸代码



三、实例爬虫项目实战
---根据以上分析进一步分步骤写代码

1、导入相关的包

2、批量获取id值

### 批量获取id值
 url='http://scxk.nmpa.gov.cn:81/xk/itownet/portalAction.do?method=getXkzsList'
### UA伪装
 headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36'
    }
 data={'on':'true','page':1,'pageSize':15,'productNam':'','conditionType':'1','applyname':'','applysn':''  }

### 发起请求
json_ids=requests.post(url=url,headers=headers,data=data).json()
### 遍历存储
      for dic in json_ids['list']:
           id_list.append(dic['ID'])
3、获取企业详情数据，并将id拼接

post_url='http://scxk.nmpa.gov.cn:81/xk/itownet/portalAction.do?method=getXkzsById'
    for id in id_list:
        data={'id':id}
        detail_json= requests.post(url=post_url,headers=headers,data=data).json()
        all_data_list.append(detail_json)


4、持久化存储

 ### 持久化存储
    fp=open('./allData.json','w',encoding='utf-8')
    json.dump(all_data_list,fp=fp,ensure_ascii=False)
以上呢写的是爬取第一页的数据，优化后爬取所有页面数据的完整代码见如下

    import requests
    import json
    import os
    if __name__ == '__main__':
        # 批量获取id值
        url='http://scxk.nmpa.gov.cn:81/xk/itownet/portalAction.do?method=getXkzsList'
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36'
        }
        # 获取存储企业id
        id_list = []
        # 存储所有企业的详情数据
        all_data_list = []
        # 动态获取所有页面数据
        for page in range(1,10):
            page=str(page)
            data={'on':'true','page':page,'pageSize':15,'productNam':'','conditionType':'1','applyname':'','applysn':''  }
            json_ids=requests.post(url=url,headers=headers,data=data).json()
            for dic in json_ids['list']:
               id_list.append(dic['ID'])
               print(id_list)
        # 获取企业详情数据
        post_url='http://scxk.nmpa.gov.cn:81/xk/itownet/portalAction.do?method=getXkzsById'
        for id in id_list:
            data={'id':id}
            detail_json= requests.post(url=post_url,headers=headers,data=data).json()
            all_data_list.append(detail_json)
        # 持久化存储
        fp=open('./allData.json','w',encoding='utf-8')
        json.dump(all_data_list,fp=fp,ensure_ascii=False)

一、正则表达式基础
---练习正则表达式的小网站

https://regex101.com/

1、.表示除了换行符以外的任何单个字符

例如我们想找到拜字前面的字符



2、*表示重复匹配任意次包括0次

例如我们想找到，后面的全部字符，他就会匹配从，开始的全部字符



3、+表示重复匹配任意次，不包括0次，需要跟*区别开，那么同样的例子，用+会有什么结果呢，是一样的结果，但是我们看下面这个例子就能体会不一样的地方了。

如果没有，+就不会匹配到字符，如果是*就可以匹配到因为*包含0次，+不包含0次



4、{}匹配指定出现的次数，{n}、X{m，n}，表示X字符连续出现的次数至少m次，至多n次



5、贪婪模式和非贪婪模式

贪婪模式：尽可能多的匹配，例如：*、+

非贪婪模式：精准匹配，例如：.+?

例如：我们想匹配下面所有的html标签内容

source = '<html><head><title>Title</title>'
1）如果用<.*>呢，我们发现他是尽可能多的匹配到所有并且都打印了



2）我们再来看看.*？，我们就可以分别匹配到了，这就是简单一个例子体会贪婪模式和非贪婪模式



6、转义符\

如果我们想匹配下面字段.前面的字符



按照之前的学的内容来看，匹配到的内容是不对的，是因为.*等这些字符是元字符，我们需要转义符/,所以更改后结果是



7、匹配某种字符类型

\d 匹配0-9之间任意一个数字字符，等价于表达式 [0-9]
\D 匹配任意一个不是0-9之间的数字字符，等价于表达式 [^0-9]
\s 匹配任意一个空白字符，包括 空格、tab、换行符等，等价于表达式 [\t\n\r\f\v]
\S 匹配任意一个非空白字符，等价于表达式 [^ \t\n\r\f\v]
\w 匹配任意一个文字字符，包括大小写字母、数字、下划线，等价于表达式 [a-zA-Z0-9_]，缺省情况也包括 Unicode文字字符，如果指定 ASCII 码标记，则只包括ASCII字母
\W 匹配任意一个非文字字符，等价于表达式 [^a-zA-Z0-9_]
反斜杠也可以用在方括号里面，比如 [\s,.] 表示匹配 ： 任何空白字符， 或者逗号，或者点
8、方括号[]匹配几个字符之一

[abc] 可以匹配 a, b, 或者 c 里面的任意一个字符。等价于 [a-c] 。
[a-c] 中间的 - 表示一个范围从a 到 c。
如果你想匹配所有的小写字母，可以使用 [a-z]
[.a*]，当元字符出现在[]中就相当于匹配这个字符，而失去了元字符本身的意义，在这里他只是一个字符
9、^ 这个字符表示文本开头位置，如果正则表达式设定为单行模式、表示匹配整个文本的开头位置，多行模式，匹配文本每行的开头位置

10、$表示匹配文本的结尾位置，如果正则表达式设定为单行模式、表示匹配整个文本的结尾位置，多行模式，匹配文本每行的结尾位置

11、|表示匹配其中之一

12、（）-分组

例如我们想从文本中提取每个人的姓名和对应手机号：

13、字符串切割spit（）、replace（）

# 二、正则表达式爬虫实例分析

---需求：爬取糗事百科的图片数据  

https://www.qiushibaike.com/imgrank/

---需求拆解

回顾数据解析的步骤：

1）指定标签的定位

2）标签或者标签对应属性中存储的数据进行提取（解析）

1、我们看下想爬取图片的源码，发现图片地址是存在 class属性=thumb下面的 img标签下，所以我们还是需要先进行第一步，获取到所有html文本数据

```python
url = 'https://www.qiushibaike.com/imgrank/'
headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.66 Safari/537.36'}
page_text = requests.get(url=url1, headers=headers).text

```

2、通过正则表达式解析出源码中src值，通过findall方法取出并存在列表中

```python
ex = '<div class="thumb">.*?<img src="(.*?)" alt=.*?</div>'
image_src_list = re.findall(ex, page_text, re.S)
```

3、我们观察我们取到的并不是完整的图片地址，需要拼接https，然后最终通过requests.get方法获取图片数据

```python
    for src in image_src_list:
    # 拼接出一个完整的图片url
      src = 'https:' + src
    # 获取二进制数据，请求到了图片的二进制数据
      image_data = requests.get(url=src, headers=headers).content
```


4、接下来就是存储啦，那我们需要新建个文件夹存储图片，这样他就有家了

```python
 if not os.path.exists('./qiutuLibs'):
        os.mkdir('./qiutuLibs')
```

5、接下来需要考虑图片的命名，以什么名字存储呢？截取图片src最后的地址吧进行存储



```python
src_name = src.split('/')[-1]
          imgPath = './qiutuLibs/' + src_name
          with open(imgPath, 'wb') as fp:
             fp.write(image_data)
             print('下载成功')
```

---思考？这样我们就获取到了第一页的数据，但是如果想获取到所有页面的数据，怎么办呢？

6、我们发现我们跳转页面，url后面的参数只有page，跟着page的具体是第几页，所以我们可以设置个变量，在通过循环调用我们想爬取多少页，就可以多少页啦，最后的结果直接附上波波老师教导下的完整代码喽



三、爬虫实例---正则表达式---完整代码


```python
import requests
import re
import os

if __name__ == '__main__':
    # 创建文件夹保存所有图片
    if not os.path.exists('./qiutuLibs'):
        os.mkdir('./qiutuLibs')
    url = 'https://www.qiushibaike.com/imgrank/'
    # 对应爬取所有页码的图片数据
    # https://www.qiushibaike.com/imgrank/page/13/
    # 设定一个通用的url的模板
    url1 = 'https://www.qiushibaike.com/imgrank/page/%d/'
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.66 Safari/537.36'}
    for page_num in range(1, 3):
        new_url = format(url1 % page_num)
        print(new_url)
     # 使用聚焦爬虫，爬取所有图片
        # 如何从当前源码解析到image标签对应的src的值? 经过分析  抽取class=thumb
        page_text = requests.get(url=url1, headers=headers).text

        ex = '<div class="thumb">.*?<img src="(.*?)" alt=.*?</div>'
    # re.S 多行
        image_src_list = re.findall(ex, page_text, re.S)
    # print(image_src_list)
        for src in image_src_list:
        # 拼接出一个完整的图片url
          src = 'https:' + src
        # 获取二进制数据，请求到了图片的二进制数据
          image_data = requests.get(url=src, headers=headers).content
        # 进行持久化存储
        # 生成图片名称，截取图片地址中的名称
          src_name = src.split('/')[-1]
          imgPath = './qiutuLibs/' + src_name
          with open(imgPath, 'wb') as fp:
             fp.write(image_data)
             print('下载成功')
```

# 三、bs4数据解析

### 一、bs4数据解析原理

1、数据解析原理：

-标签定位
-提取解析标签属性存储的数据值

2、bs4数据解析原理：

- 实例化beautifulSoup对象，并且将页面源码数据加载到该对象中
- 通过调用beautifulSoup对象中相关的属性或者方法进行标签定位和数据提取

3、实例化的两种方法

- 将本地html文档加载到该对象中

fp=open('./exclusion_bs4_test.html','r',encoding='utf-8')

soup=BeautifulSoup(fp,'lxml')

print(soup)

- 将互联网页面数据加载到对象中

page_text=response.text
    soup=BeautifulSoup(page_text,'lxml')

4、根据页面元素的方法和属性定位

1）方法

- soup.tagName:返回的是文档中第一次出现的tagName的标签
   tagName：div...
- soup.find('div'):返回的是文档中第一次出现的div
- soup.find('div',class_/id/其他属性值='xx')定位class=xx/id=XX的div
- soup.findall()  返回符合要求的所有标签
- soup.select('某种选择器（id、class,标签选择器)'),返回的是一个列表
- 层级选择器：soup.select('.xxxclassname>ul a')[i] >表示一个层级，空格表示多个层级

2）获取标签中文本数据属性值

- soup.a标签.text、get_text()、string

区别：前两个可以获取某一个标签中所有的文本内容，即使文本内容不属于该标签直系文本内容，而string只能获取该标签下直系的 文本内容

### 二、bs4数据解析实例分析

--需求：爬取三国演义小说所有章节标题和章节内容

https://www.shicimingju.com/book/sanguoyanyi.html

1、f12分析所有章节标题所在标签，在class=“book-mulu”，下的ul标签下的li标签下，获取这个文本数据就可以得到标题

![img](https://img-blog.csdnimg.cn/20210103103628392.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1Nzc5Nzk0,size_16,color_FFFFFF,t_70)

所以写下来就是这样的

```routeros
 url='https://www.shicimingju.com/book/sanguoyanyi.html'
  headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.66 Safari/537.36'}
# 拿到页面数据
  html_text=requests.get(url=url,headers=headers).text
# 数据解析，bs4 解析
# 实例化 beautifulsoup对象
  soup=BeautifulSoup(html_text,'lxml')
  li_list=soup.select('.book-mulu>ul>li')
 for li in li_list:
      # a标签直系文本内容，章节标题
      title=li.a.string
```

2、分析章节内容所在位置，如何解析

![img](https://img-blog.csdnimg.cn/20210103103929944.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1Nzc5Nzk0,size_16,color_FFFFFF,t_70)

第一点：我们发现详情页的url是，我们获取到标题的href标签的链接

第二点:我们发现获取到整个页面元素后，只有获取到class=chapter_content标签下文本我们就可以爬取章节内容啦

```python
 detail_url='https://www.shicimingju.com'+li.a['href']
      # 对详情页发起请求，请求章节内容
      detail_page_text=requests.get(url=detail_url,headers=headers).text
      # 解析内容
      detail_soup= BeautifulSoup(detail_page_text, 'lxml')
      div_tag=detail_soup.find('div',class_='chapter_content')
      # 解析章节内容
      content=div_tag.text
```

### 三、完整爬取代码

```python
from bs4 import BeautifulSoup
import requests
 
if __name__ == '__main__':
  # 需求：爬取三国演义小说所有章节标题和章节内容
  url='https://www.shicimingju.com/book/sanguoyanyi.html'
  headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.66 Safari/537.36'}
# 拿到页面数据
  html_text=requests.get(url=url,headers=headers).text
# 数据解析，bs4 解析
# 实例化 beautifulsoup对象
  soup=BeautifulSoup(html_text,'lxml')
  li_list=soup.select('.book-mulu>ul>li')
  fp=open('./sanguo.txt','w',encoding='utf-8')
  for li in li_list:
      # a标签直系文本内容，章节标题
      title=li.a.string
      print(title)
      detail_url='https://www.shicimingju.com'+li.a['href']
      # 对详情页发起请求，请求章节内容
      detail_page_text=requests.get(url=detail_url,headers=headers).text
      # 解析内容
      detail_soup= BeautifulSoup(detail_page_text, 'lxml')
      div_tag=detail_soup.find('div',class_='chapter_content')
      # 解析章节内容
      content=div_tag.text
      print(content)
      fp.write(title+':'+content+'\n')
      print(title,"爬取成功！")
```

# 四、xpath数据解析

### 一、[Xpath](https://so.csdn.net/so/search?q=Xpath&spm=1001.2101.3001.7020)解析基础

1、xpath[表达式](https://so.csdn.net/so/search?q=表达式&spm=1001.2101.3001.7020)（层级）

- -/:表示的是从根结点开始定位。一个/表示一个层级。
- -//:表示的是多个层级。可以从任意位置开始定位。
- -属性定位://meta[@charset=“utf-8”]
- 通用写法://tag[@attrName=“attrValue”]
- -索引定位://tag[@attrName=“attrValue”]/tag[n]索引是从1开始的
- -取文本:/text():获取的是标签中直系的文本内容
- //text():获取的是标签中非直系的文本内容
- -取属性:/@attrName

2、Xpath解析数据基本步骤

- 实例化一个etree对象，并且将被解析的页面源码数据加载到该对象中

1）将本地的html文档中的源码数据加载到etree对象中

etree.parse（filepath）

2）将互联网上获取的源码数据加载到该对象中

etree.Html('pagetext')

- 通过调用etree对象中的xpath方法结合着xpath表达式实现标签定位和内容的捕获



### 二、根据Xpath定位爬取【彼岸图网图片】实战分析记录

--需求：爬取彼岸图网的任意图片，我选择爬取的美食图片

--url  http://pic.netbian.com/4kmeishi/

1、第一步：我们还是老步骤，我们需要先获取到页面源码数据，也就是根据requests的方法获取，对了不要忘记进行UA伪装哦！

```routeros
import requests
 
url = 'http://pic.netbian.com/4kmeishi/'
 
headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.66 Safari/537.36'}
response = requests.get(url=url, headers=headers)
page_text=response.text
```

2、第二步就是我们上面基础步骤中的实例etree对象，那么首先我们需要导入库，然后再根据上面我们介绍的方法进行实例化对象，下面是具体的代码~

```abnf
from lxml import etree
tree = etree.HTML(page_text)
```

3、第三步就是重头戏了，我们要进行xpath定位图片了，打入敌人内部，小F12安排上，我们看下面的截图，很容易发现，我们的目标是解析到class=slist下面的ul下面的li标签下面的src属性，再拼接上url就可以获取到图片了，那么图片我们需要进行存储，这里就用alt属性进行图片命名吧，下面我们再进一步分析

![img](https://img-blog.csdnimg.cn/20210107203444351.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1Nzc5Nzk0,size_16,color_FFFFFF,t_70)

首先我们要做一个局部解析，也就是先定位到li标签，再循环这个li-list，获取到当前页面所有的src，src是第一个属性值所以[0]定位，在进行拼接，也就是这样

```nginx
# 解析src的属性值、图片命名，url拼接，解析alt属性值
 li_list = tree.xpath('//div[@class="slist"]/ul/li')
 for li in li_list:
        # 对第一个做局部解析
        image_src = 'http://pic.netbian.com/' + li.xpath('./a/img/@src')[0]
```

然后我们来对图片进行命名，命名是中文的，而且我们遇到了中文乱码，后面具体再说[爬虫](https://so.csdn.net/so/search?q=爬虫&spm=1001.2101.3001.7020)遇到中文乱码的解决办法吧，先按照我们的步骤往下走

```stylus
image_name = li.xpath('./a/img/@alt')[0] + '.jpg'
```

下一步就是获取图片的二进制数据，进行持久化存储了，这里是我们的老朋友了，就不多说了，直接上部分代码，后面我再贴个完整的代码哈

```livecodeserver
image_data=requests.get(url=image_src,headers=headers).content
        image_path='picLibs/'+image_name
        with open(image_path,'wb') as fp:
            fp.write(image_data)
            print(image_name,"下载成功")
        fp.close()
```

最后呢，我们存储数据，发现图片的命名真的乱码了，但是用波波老师的方法并没有解决我的中文乱码问题，而是用另一个同学的办法解决的，下面部分说下解决办法

### 三、爬取数据过程中，中文乱码的解决办法

1、获取响应数据，设置响应数据的encoding

```python
  # 解决乱码
    response.encoding=response.apparent_encoding
    page_text=response.text
```

### 四、完整代码

```python
if __name__ == '__main__':
    url = 'http://pic.netbian.com/4kmeishi/'
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.66 Safari/537.36'}
    response = requests.get(url=url, headers=headers)
    # 获取响应对象
    # 解决乱码
    response.encoding=response.apparent_encoding
    page_text=response.text
    # 数据解析
    tree = etree.HTML(page_text)
    # 解析src的属性值、图片命名，url拼接，解析alt属性值
    li_list = tree.xpath('//div[@class="slist"]/ul/li')
    if not os.path.exists('./picLibs'):
        os.mkdir('./picLibs')
    for li in li_list:
        # 对第一个做局部解析
        image_src = 'http://pic.netbian.com/' + li.xpath('./a/img/@src')[0]
        image_name = li.xpath('./a/img/@alt')[0] + '.jpg'
        image_name = image_name.encode("utf-8").decode("utf-8")
        # print(image_name, image_src)
        image_data=requests.get(url=image_src,headers=headers).content
        image_path='picLibs/'+image_name
        with open(image_path,'wb') as fp:
            fp.write(image_data)
            print(image_name,"下载成功")
        fp.close()
```